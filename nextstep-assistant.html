<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>NextStep Assistant — Blue Theme</title>

<!-- Blue-themed styling for NextStep -->
<style>
  :root{
    --bg:#e9f2ff;
    --primary:#0b63d6;
    --primary-600:#0754b6;
    --accent:#0d86ff;
    --muted:#6b7b8f;
    --card:#ffffff;
    --glass: rgba(255,255,255,0.85);
    --radius:14px;
    --shadow: 0 8px 30px rgba(11,99,214,0.08);
    font-family: Inter, system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
  }

  /* Chat container */
  .ns-chat {
    position: fixed;
    right: 20px;
    bottom: 24px;
    width: 360px;
    max-width: calc(100% - 40px);
    box-shadow: var(--shadow);
    border-radius: var(--radius);
    background: linear-gradient(180deg,var(--glass), #f6fbff);
    overflow: hidden;
    display: flex;
    flex-direction: column;
    border: 1px solid rgba(11,99,214,0.08);
  }

  /* header */
  .ns-header {
    display:flex;
    gap:12px;
    align-items:center;
    padding:12px 14px;
    background: linear-gradient(90deg,var(--primary),var(--primary-600));
    color:white;
  }
  .ns-logo {
    width:40px;height:40px;border-radius:10px;
    background:rgba(255,255,255,0.12);
    display:flex;align-items:center;justify-content:center;
    font-weight:700;font-size:18px;
    box-shadow: inset 0 -4px 10px rgba(0,0,0,0.06);
  }
  .ns-title { font-size:15px; font-weight:600; }
  .ns-sub { font-size:12px; opacity:0.9 }

  /* body */
  .ns-body {
    padding:12px;
    display:flex;
    flex-direction:column;
    gap:8px;
    min-height:280px;
    max-height:420px;
    overflow:auto;
    background: transparent;
  }
  .msg { max-width:82%; padding:10px 12px; border-radius:12px; line-height:1.3; }
  .msg.user { margin-left:auto; background:linear-gradient(180deg,#e6f0ff,#cfe6ff); color: #072c5f; border-bottom-right-radius:6px;}
  .msg.assistant { margin-right:auto; background: #ffffff; color:#07203a; border-bottom-left-radius:6px; box-shadow: 0 2px 8px rgba(11,99,214,0.03); }

  /* helper text */
  .ns-hint { font-size:12px; color:var(--muted); padding:6px 12px; }

  /* input area */
  .ns-input-row {
    display:flex; gap:8px; padding:12px; align-items:center;
    border-top:1px solid rgba(11,99,214,0.04);
    background: linear-gradient(180deg, rgba(255,255,255,0.6), rgba(255,255,255,0.8));
  }
  .ns-input {
    flex:1; padding:10px 12px; border-radius:10px; border:1px solid rgba(11,99,214,0.09);
    outline:none; font-size:14px; background:transparent;
  }
  .ns-send {
    background:var(--primary); color:white; border:none; padding:9px 12px; border-radius:10px;
    font-weight:600; cursor:pointer;
  }
  .ns-send:active { transform:translateY(1px); }

  /* small screens: shrink widget */
  @media (max-width:420px){
    .ns-chat{ right:12px; left:12px; bottom:12px; width:auto; border-radius:12px; }
  }

  /* accessibility focus */
  .ns-send:focus, .ns-input:focus { box-shadow: 0 0 0 4px rgba(11,99,214,0.12); }
</style>
</head>
<body>

<!-- Chat widget -->
<div class="ns-chat" role="region" aria-label="NextStep Assistant">
  <div class="ns-header">
    <div class="ns-logo" aria-hidden="true">NS</div>
    <div>
      <div class="ns-title">NextStep Assistant</div>
      <div class="ns-sub">Civic help, events, and guidance</div>
    </div>
  </div>

  <div id="nsBody" class="ns-body" aria-live="polite">
    <!-- Messages will appear here -->
    <div class="ns-hint" id="nsHint">Hi — ask me about voting, local resources, or how to use NextStep.</div>
  </div>

  <div class="ns-input-row">
    <input id="nsInput" class="ns-input" placeholder="Ask NextStep Assistant..." aria-label="Message input" />
    <button id="nsSend" class="ns-send" aria-label="Send message">Send</button>
  </div>
</div>

<!-- Script: attempt WebLLM, otherwise fallback rule-based assistant -->
<script>
(async function () {
  // Assistant personality (system prompt) for guiding model responses
  const SYSTEM_PROMPT = `You are NextStep Assistant, a friendly, factual civic-engagement helper for the NextStep platform. 
Be concise, clear, and avoid giving legal advice. When asked for external resources, suggest verifiable sources like government websites.
If you don't know, say "I don't know" and offer a suggestion for where to check. Limit responses to ~220 words.`;

  const nsBody = document.getElementById('nsBody');
  const nsInput = document.getElementById('nsInput');
  const nsSend  = document.getElementById('nsSend');
  const nsHint = document.getElementById('nsHint');

  function addMessage(text, who='assistant'){
    const p = document.createElement('div');
    p.className = 'msg ' + (who === 'user' ? 'user' : 'assistant');
    p.innerText = text;
    nsBody.appendChild(p);
    nsBody.scrollTop = nsBody.scrollHeight;
  }

  // Basic rule-based fallback (100% client-side, no model)
  function fallbackReply(userText) {
    const normalized = userText.trim().toLowerCase();
    if (!normalized) return "Say something and I'll try to help!";
    // Common Qs for civic engagement
    if (normalized.includes('vote') || normalized.includes('voting') || normalized.includes('register')) {
      return "To register to vote, check your state's official election website or usa.gov/voting for guidance. If you tell me your state, I can give more specific steps.";
    }
    if (normalized.includes('who is my representative') || normalized.includes('representative')) {
      return "To find your elected officials, use the Google Civic Information API or your state's 'Find My Representative' tool. On NextStep we can add a lookup by zip code — do you want me to show how?";
    }
    if (normalized.includes('events') || normalized.includes('volunteer')) {
      return "You can find local civic events by checking your city or county website, local nonprofits, or volunteer platforms like volunteermatch.org. On NextStep we can integrate an events feed.";
    }
    if (normalized.includes('how to use') || normalized.includes('navigate')) {
      return "Tell me which page you're on (events, volunteering, contact your rep) and I can guide you step-by-step.";
    }
    if (normalized.includes('help') || normalized.includes('hi') || normalized.includes('hello')) {
      return "Hi! I'm NextStep Assistant. I can answer civic questions, help find resources, and guide you through the site. What would you like to do?";
    }
    // default fallback
    return "I don't have a precise answer right now. Try asking for 'voter registration', 'find representatives', or 'volunteer events'.";
  }

  // Try to load a browser LLM library (example: @mlc-ai/web-llm)
  // If it fails or isn't available, we use the fallbackReply above.
  let modelClient = null;
  let usingModel = false;

  async function tryLoadWebLLM() {
    // Attempt to import a library from CDN if available
    // NOTE: this code tries to support common WebLLM loaders; it will silently fail to the fallback if not present.
    try {
      // Prefer dynamic import if the environment supports it:
      if (typeof window.WebLLM !== 'undefined') {
        console.log("Found WebLLM global.");
        return window.WebLLM;
      }
      // Try to load MLC Web LLM from unpkg. If unavailable, this will throw.
      await new Promise((resolve, reject) => {
        const s = document.createElement('script');
        s.src = "https://unpkg.com/@mlc-ai/web-llm@latest/dist/web-llm.min.js";
        s.onload = resolve;
        s.onerror = reject;
        document.head.appendChild(s);
        setTimeout(() => {
          // don't hang forever
          if (typeof window.WebLLM === 'undefined') {
            // still not loaded after short wait -> reject
            reject(new Error('web-llm not available'));
          } else resolve();
        }, 1200);
      });
      if (typeof window.WebLLM === 'undefined') throw new Error('web-llm loaded but API missing');
      return window.WebLLM;
    } catch (err) {
      console.warn("WebLLM load failed:", err);
      return null;
    }
  }

  // Chat with the model if available
  async function askModel(question) {
    if (!modelClient) throw new Error("Model client not ready");
    // The exact API for browser LLM libraries varies. Below is a common pattern:
    // - load a model once
    // - call .generate or .chat with a system prompt + user message
    // Since there are many implementations, we try a few common call patterns safely.
    try {
      // pattern 1: modelClient.chat({messages: [...]})
      if (typeof modelClient.chat === 'function') {
        const resp = await modelClient.chat([
          {role:'system', content: SYSTEM_PROMPT},
          {role:'user', content: question}
        ]);
        if (resp && resp.choices && resp.choices[0] && resp.choices[0].message) {
          return resp.choices[0].message.content;
        } else if (typeof resp === 'string') {
          return resp;
        } else if (resp && resp.text) {
          return resp.text;
        }
      }
      // pattern 2: modelClient.generate
      if (typeof modelClient.generate === 'function') {
        const out = await modelClient.generate(SYSTEM_PROMPT + "\n\nUser: " + question + "\nAssistant:");
        if (out && out.text) return out.text;
        if (typeof out === 'string') return out;
      }
      // pattern 3: modelClient.predict
      if (typeof modelClient.predict === 'function') {
        const out = await modelClient.predict(question);
        if (out && out.generated_text) return out.generated_text;
        if (typeof out === 'string') return out;
      }
    } catch (err) {
      console.warn("Model chat failed:", err);
    }
    throw new Error("Model API not supported by client");
  }

  // Initialize and attempt to load a small in-browser model (if available)
  // NOTE: For many setups you'll need to host the model files or have a browser runtime (WebGPU) configured.
  // We try to be safe: don't block UI if model load is slow.
  (async () => {
    const WebLLM = await tryLoadWebLLM();
    if (!WebLLM) {
      nsHint.innerText = "Running in offline fallback mode (no in-browser model found).";
      usingModel = false;
      return;
    }

    nsHint.innerText = "Found in-browser LLM library — attempting to load a model (may take a while).";
    try {
      // The following is an example pattern for mlc-ai/web-llm or similar libraries.
      // Adjust model id/path to one you have or that the library supports in-browser.
      // Example: small quantized model hosted locally or via CDN.
      // We attempt a very conservative default that is likely to fail unless you host model files:
      // modelClient = await WebLLM.load({ model: "gpt2" });
      // Instead we try a safe "auto" loader if provided:
      if (typeof WebLLM.load === 'function') {
        // Try to load a tiny model if available on the runtime. Replace "gpt2" with your model path if you host one.
        try {
          modelClient = await WebLLM.load({ model: "gpt2" });
        } catch (e) {
          // try alternative
          modelClient = await WebLLM.load({ model: "lama-mini" }).catch(()=>null);
        }
      } else if (typeof WebLLM.create === 'function') {
        modelClient = await WebLLM.create();
      } else {
        modelClient = WebLLM;
      }

      if (modelClient) {
        usingModel = true;
        nsHint.innerText = "NextStep Assistant: in-browser model ready.";
        console.log("Model client ready:", modelClient);
      } else {
        usingModel = false;
        nsHint.innerText = "Could not initialize an in-browser model — running fallback.";
      }
    } catch (err) {
      console.warn("Error initializing model:", err);
      usingModel = false;
      nsHint.innerText = "Model initialization error — running fallback mode.";
    }
  })();

  // Handle user sending a message
  async function handleSend() {
    const text = nsInput.value.trim();
    if (!text) return;
    addMessage(text, 'user');
    nsInput.value = '';
    // Show a typing placeholder
    const typing = document.createElement('div');
    typing.className = 'msg assistant';
    typing.innerText = 'Thinking...';
    nsBody.appendChild(typing);
    nsBody.scrollTop = nsBody.scrollHeight;

    try {
      if (usingModel && modelClient) {
        // model available — ask it
        let reply = await askModel(text); // may throw
        if (!reply) reply = "Hmm — I couldn't generate a reply.";
        typing.innerText = reply;
      } else {
        // fallback rule-based
        const reply = fallbackReply(text);
        typing.innerText = reply;
      }
    } catch (err) {
      console.warn("Error during reply:", err);
      typing.innerText = "Sorry — there was an error generating a response. Running fallback reply.";
      // fallback content
      const reply = fallbackReply(text);
      setTimeout(()=>{ typing.innerText = reply; }, 600);
    }
    nsBody.scrollTop = nsBody.scrollHeight;
  }

  // keyboard enter
  nsInput.addEventListener('keydown', (e) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  });
  nsSend.addEventListener('click', handleSend);

  // OPTIONAL: sample welcome message
  setTimeout(()=> {
    addMessage("Welcome to NextStep Assistant — I can help with voter registration, finding local civic events, and navigating the NextStep platform. Try: 'How do I register to vote?' or 'Find volunteer events near me.'");
  }, 400);

})();
</script>
<!-- Firebase Auth with Profile Dropdown -->
<script type="module" src="auth-check.js"></script>
</body>
</html>
